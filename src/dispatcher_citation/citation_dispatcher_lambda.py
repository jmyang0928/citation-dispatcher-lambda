import json
import os
import boto3
import pandas as pd
import io
import re

# å¾ç’°å¢ƒè®Šæ•¸ç²å–é…ç½®
SQS_QUEUE_URL = os.environ.get('SQS_QUEUE_URL')
S3_BUCKET_NAME = os.environ.get('S3_BUCKET_NAME')
INPUT_S3_PREFIX = os.environ.get('INPUT_S3_PREFIX', 'cleaned_data/')

# åˆå§‹åŒ– AWS å®¢æˆ¶ç«¯
s3 = boto3.client('s3')
sqs = boto3.client('sqs')
lambda_client = boto3.client('lambda')

def handler(event, context):
    """
    Lambda è™•ç†å‡½å¼ã€‚
    æƒæ S3 ä¸ŠæŒ‡å®šå‰ç¶´çš„æ‰€æœ‰ .jsonl æª”æ¡ˆï¼Œå¾æœ€æ–°çš„æœˆä»½é–‹å§‹ï¼Œ
    ä¸¦å°‡æ¯ä¸€è¡Œè½‰æ›ç‚ºä¸€å€‹ SQS è¨Šæ¯ã€‚
    - å¦‚æœåŸ·è¡Œæ™‚é–“å³å°‡è€—ç›¡ï¼Œæ­¤å‡½å¼æœƒè§¸ç™¼è‡ªå·±ä»¥æ¥çºŒåŸ·è¡Œã€‚
    - æ”¯æ´æ¸¬è©¦æ¨¡å¼ï¼Œåƒ…ç™¼é€ä¸€ç­†è³‡æ–™ã€‚
    """
    if not SQS_QUEUE_URL or not S3_BUCKET_NAME:
        print("éŒ¯èª¤ï¼šç’°å¢ƒè®Šæ•¸ SQS_QUEUE_URL æˆ– S3_BUCKET_NAME æœªè¨­å®šã€‚")
        return {'statusCode': 500, 'body': 'ç’°å¢ƒè®Šæ•¸æœªè¨­å®š'}

    # --- æ¸¬è©¦æ¨¡å¼é‚è¼¯ ---
    if event.get('test_mode') is True:
        return run_test_mode()

    # --- å¸¸è¦åŸ·è¡Œé‚è¼¯ ---
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ä¾†è‡ªä¸Šä¸€æ¬¡åŸ·è¡Œçš„æ¥çºŒç‹€æ…‹
    start_index = event.get('start_index', 0)
    all_files = event.get('file_list')

    # å¦‚æœé€™æ˜¯é¦–æ¬¡åŸ·è¡Œï¼Œç²å–ä¸¦æ’åºå®Œæ•´çš„æª”æ¡ˆåˆ—è¡¨
    if all_files is None:
        print("é¦–æ¬¡åŸ·è¡Œï¼šæ­£åœ¨ç²å–ä¸¦æ’åºæ‰€æœ‰æª”æ¡ˆåˆ—è¡¨...")
        all_files = []
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=S3_BUCKET_NAME, Prefix=INPUT_S3_PREFIX)
        for page in pages:
            if 'Contents' in page:
                for obj in page['Contents']:
                    if obj['Key'].endswith('.jsonl'):
                        all_files.append(obj['Key'])
        # åå‘æ’åºï¼Œç¢ºä¿å¾æœ€æ–°çš„æœˆä»½ (ä¾‹å¦‚ 'papers_2405.jsonl') é–‹å§‹è™•ç†
        all_files.sort(reverse=True)
        print(f"æ‰¾åˆ° {len(all_files)} å€‹æª”æ¡ˆã€‚å°‡å¾æœ€æ–°çš„æœˆä»½é–‹å§‹è™•ç†ã€‚")

    # æ±ºå®šæœ¬æ¬¡åŸ·è¡Œè¦è™•ç†çš„æª”æ¡ˆ
    files_to_process = all_files[start_index:]
    if not files_to_process:
        print("æ²’æœ‰æ›´å¤šæª”æ¡ˆéœ€è¦è™•ç†ã€‚æµç¨‹å®Œæˆã€‚")
        return {'statusCode': 200, 'body': 'æ‰€æœ‰æª”æ¡ˆéƒ½å·²åˆ†æ´¾ã€‚'}
    
    total_jobs_dispatched = 0

    try:
        for idx, file_key in enumerate(files_to_process):
            current_file_index = start_index + idx
            
            # æª¢æŸ¥å‰©é¤˜åŸ·è¡Œæ™‚é–“
            if context.get_remaining_time_in_millis() < 30000:
                print("æ™‚é–“å³å°‡è€—ç›¡ï¼Œè§¸ç™¼ä¸‹ä¸€å€‹ Lambda æ¥çºŒåŸ·è¡Œã€‚")
                next_start_index = current_file_index # ä¸‹æ¬¡å¾ç•¶å‰æª”æ¡ˆé–‹å§‹
                new_payload = {'start_index': next_start_index, 'file_list': all_files}
                
                lambda_client.invoke(
                    FunctionName=context.function_name,
                    InvocationType='Event', # éåŒæ­¥è§¸ç™¼
                    Payload=json.dumps(new_payload)
                )
                print(f"å·²æˆåŠŸè§¸ç™¼ä¸‹ä¸€å€‹å‡½å¼ï¼Œå°‡å¾æª”æ¡ˆç´¢å¼• {next_start_index} é–‹å§‹ã€‚")
                return {'statusCode': 200, 'body': 'å¾…ä¸‹ä¸€æ¬¡åŸ·è¡Œæ¥çºŒ...'}

            print(f"æ­£åœ¨è™•ç†æª”æ¡ˆ #{current_file_index + 1}/{len(all_files)}: {file_key}...")
            
            file_obj = s3.get_object(Bucket=S3_BUCKET_NAME, Key=file_key)
            df = pd.read_json(io.BytesIO(file_obj['Body'].read()), lines=True)
            
            if 'id' in df.columns and 'title' in df.columns:
                df_subset = df[['id', 'title']].dropna()
                dispatched_in_file = dispatch_dataframe(df_subset)
                total_jobs_dispatched += dispatched_in_file
            else:
                print(f"  [è­¦å‘Š] æª”æ¡ˆ {file_key} ç¼ºå°‘ 'id' æˆ– 'title' æ¬„ä½ï¼Œå·²è·³éã€‚")

        print("\n--- åˆ†æ´¾å®Œæˆ ---")
        print(f"æœ¬æ¬¡åŸ·è¡Œéˆç¸½å…±è™•ç†äº† {len(files_to_process)} å€‹æª”æ¡ˆã€‚")
        print(f"ç¸½å…±ç™¼é€äº† {total_jobs_dispatched} å€‹ä»»å‹™åˆ° SQSã€‚")
        
        return {'statusCode': 200, 'body': f'æˆåŠŸå¾ {len(files_to_process)} å€‹æª”æ¡ˆä¸­åˆ†æ´¾äº† {total_jobs_dispatched} å€‹ä»»å‹™ã€‚'}

    except Exception as e:
        print(f"åˆ†æ´¾éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        raise e

def run_test_mode():
    """åŸ·è¡Œæ¸¬è©¦æ¨¡å¼ï¼Œåƒ…åˆ†æ´¾ä¸€ç­†è³‡æ–™ã€‚"""
    print("ğŸš€ --- åŸ·è¡Œæ¸¬è©¦æ¨¡å¼ --- ğŸš€")
    paginator = s3.get_paginator('list_objects_v2')
    # ç²å–ä»»ä½•ä¸€å€‹ .jsonl æª”æ¡ˆ
    pages = paginator.paginate(Bucket=S3_BUCKET_NAME, Prefix=INPUT_S3_PREFIX)
    first_file_key = None
    for page in pages:
        if 'Contents' in page:
            for obj in page['Contents']:
                if obj['Key'].endswith('.jsonl'):
                    first_file_key = obj['Key']
                    break
            if first_file_key:
                break
    
    if first_file_key:
        print(f"æ‰¾åˆ°æ¸¬è©¦æª”æ¡ˆ: {first_file_key}")
        file_obj = s3.get_object(Bucket=S3_BUCKET_NAME, Key=first_file_key)
        # è®€å–ç¬¬ä¸€è¡Œ
        first_line = file_obj['Body'].read().splitlines()[0].decode('utf-8')
        record = json.loads(first_line)
        df = pd.DataFrame([record])
        if 'id' in df.columns and 'title' in df.columns:
            dispatch_dataframe(df[['id', 'title']])
            print("âœ… æ¸¬è©¦æ¨¡å¼å®Œæˆï¼Œå·²ç™¼é€ä¸€ç­†è³‡æ–™ã€‚")
            return {'statusCode': 200, 'body': 'æ¸¬è©¦æ¨¡å¼æˆåŠŸå®Œæˆã€‚'}
        else:
            return {'statusCode': 400, 'body': 'æ¸¬è©¦æª”æ¡ˆçš„ç¬¬ä¸€ç­†è³‡æ–™ç¼ºå°‘ id æˆ– titleã€‚'}
    else:
        print("æ‰¾ä¸åˆ°ä»»ä½•æª”æ¡ˆé€²è¡Œæ¸¬è©¦ã€‚")
        return {'statusCode': 404, 'body': 'æ‰¾ä¸åˆ°ä»»ä½•æª”æ¡ˆé€²è¡Œæ¸¬è©¦ã€‚'}

def dispatch_dataframe(df: pd.DataFrame) -> int:
    """å°‡ DataFrame ä¸­çš„æ¯ä¸€è¡Œåˆ†æ´¾åˆ° SQSã€‚"""
    if df.empty:
        return 0
    batch_size = 10
    message_count = 0
    for i in range(0, len(df), batch_size):
        chunk = df.iloc[i:i + batch_size]
        entries = []
        for _, row in chunk.iterrows():
            message_body = {'paper_id': row['id'], 'title': str(row['title'])}
            safe_id = re.sub(r'[^a-zA-Z0-9_-]', '_', str(row['id']))[:80]
            entries.append({'Id': safe_id, 'MessageBody': json.dumps(message_body)})
        if entries:
            sqs.send_message_batch(QueueUrl=SQS_QUEUE_URL, Entries=entries)
            message_count += len(entries)
    print(f"  -> å·²å¾æ­¤æª”æ¡ˆåˆ†æ´¾ {message_count} å€‹ä»»å‹™ã€‚")
    return message_count
